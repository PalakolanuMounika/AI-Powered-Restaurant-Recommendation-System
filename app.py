# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A1cDobrKOel75LtoCWc9SaN1rwJ6JruI
"""

# ==============================================================================
# Restaurant Recommendation Engine
# ==============================================================================

# Step 1: Setup and Data Loading
# ==============================================================================
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
import os

warnings.filterwarnings('ignore')

print("Loading data...")

try:
    train_customers = pd.read_csv(r'D:\PROJECTS\Restaurent Order Prediction\Train-20250726T084843Z-1-001\Train\train_customers.csv')
    train_locations = pd.read_csv(r'D:\PROJECTS\Restaurent Order Prediction\Train-20250726T084843Z-1-001\Train\train_locations.csv')
    train_orders = pd.read_csv(r'D:\PROJECTS\Restaurent Order Prediction\Train-20250726T084843Z-1-001\Train\orders.csv')
    vendors = pd.read_csv(r'D:\PROJECTS\Restaurent Order Prediction\Train-20250726T084843Z-1-001\Train\vendors.csv')
    test_customers = pd.read_csv(r'D:\PROJECTS\Restaurent Order Prediction\Test-20250726T084843Z-1-001\Test\test_customers.csv')
    test_locations = pd.read_csv(r'D:\PROJECTS\Restaurent Order Prediction\Test-20250726T084843Z-1-001\Test\test_locations.csv')
except FileNotFoundError as e:
    print("Error: One or more data files are missing.")
    raise e

# Verify required columns exist before merging
required_columns = {
    'train_customers': ['customer_id'],
    'train_orders': ['customer_id', 'vendor_id'],
    'vendors': ['id']
}

for df_name, cols in required_columns.items():
    df = eval(df_name)
    missing_cols = [col for col in cols if col not in df.columns]
    if missing_cols:
        raise KeyError(f"Missing columns {missing_cols} in {df_name}. Please check the CSV file.")

# Step 2: Data Preprocessing and Feature Engineering
# ==============================================================================
print("Starting feature engineering...")

# Merge training data
df = pd.merge(train_orders, train_customers, on='customer_id')
df = pd.merge(df, vendors, left_on='vendor_id', right_on='id')

# Customer Features
df['customer_age'] = 2025 - df['dob']
df['customer_account_age_days'] = (pd.to_datetime('2025-07-26') - pd.to_datetime(df['created_at_x'], format='mixed', dayfirst=False, errors='coerce')).dt.days

# Vendor Features
vendor_order_count = df['vendor_id'].value_counts().reset_index()
vendor_order_count.columns = ['vendor_id', 'vendor_popularity']
df = pd.merge(df, vendor_order_count, on='vendor_id', how='left')

# Time Features
df['order_date'] = pd.to_datetime(df['delivery_date'])
df['order_day_of_week'] = df['order_date'].dt.dayofweek
df['order_hour'] = pd.to_datetime(df['created_at_x'], format='mixed', dayfirst=False, errors='coerce').dt.hour

# TODO: Merge customer/vendor locations and calculate distance_km

# Step 3: Model Training
# ==============================================================================
print("Preparing data for model training...")

df['target'] = 1  # Positive samples

# TODO: Add negative samples for better model generalization

features = [
    'customer_age', 'customer_account_age_days', 'vendor_popularity',
    'order_day_of_week', 'order_hour',
    'grand_total', 'item_count', 'preparationtime'
]

categorical_features = ['gender', 'language', 'vendor_tag_name']
label_encoders = {}
for col in categorical_features:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le
        features.append(col)

X = df[features]
y = df['target']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training LightGBM model...")
lgb_params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'n_estimators': 1000,
    'learning_rate': 0.05,
    'num_leaves': 31,
    'verbose': -1,
    'seed': 42
}

model = lgb.LGBMClassifier(**lgb_params)
model.fit(X_train, y_train,
          eval_set=[(X_val, y_val)],
          eval_metric='auc',
          callbacks=[lgb.early_stopping(100)])

# Step 4: Generate Predictions for the Test Set
# ==============================================================================
print("Generating recommendations for the test set...")

# Build all customer-vendor pairs
test_customer_ids = test_customers['customer_id'].unique()
vendor_ids = vendors['id'].unique()

candidate_df = pd.MultiIndex.from_product([test_customer_ids, vendor_ids], names=["customer_id", "vendor_id"]).to_frame(index=False)

# Merge customer and vendor info
candidate_df = candidate_df.merge(test_customers, on='customer_id', how='left')
candidate_df = candidate_df.merge(vendors, left_on='vendor_id', right_on='id', how='left')

# Same features as train
candidate_df['customer_age'] = 2025 - candidate_df['dob']
candidate_df['customer_account_age_days'] = (pd.to_datetime('2025-07-26') - pd.to_datetime(candidate_df['created_at_x'], format='mixed', dayfirst=False, errors='coerce')).dt.days

candidate_df = pd.merge(candidate_df, vendor_order_count, on='vendor_id', how='left')

candidate_df['order_day_of_week'] = 5  # Assume Friday
candidate_df['order_hour'] = 13        # Assume lunch hour
candidate_df['grand_total'] = 300
candidate_df['item_count'] = 3
candidate_df['preparationtime'] = 30

for col in categorical_features:
    if col in candidate_df.columns:
        le = label_encoders[col]
        candidate_df[col] = le.transform(candidate_df[col].astype(str))

# Select only the features for prediction
X_candidate = candidate_df[features]

test_predictions = model.predict_proba(X_candidate)[:, 1]
candidate_df['prediction_score'] = test_predictions

candidate_df = candidate_df.sort_values(by=['customer_id', 'prediction_score'], ascending=[True, False])
top_recommendations = candidate_df.groupby('customer_id').first().reset_index()

# Step 5: Create Submission File
# ==============================================================================
print("Creating submission file...")

# Replace with actual location number if available
top_recommendations['CID X LOC_NUM X VENDOR'] = top_recommendations['customer_id'].astype(str) + ' X ' + \
                                               '1' + ' X ' + \
                                               top_recommendations['vendor_id'].astype(str)

submission_df = top_recommendations[['CID X LOC_NUM X VENDOR']]
submission_df.to_csv('submission1.csv', index=False)

print("Script finished. Submission saved as 'submission1.csv'.")

